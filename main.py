from langchain.callbacks.base import BaseCallbackHandler
from langchain.memory import ChatMessageHistory
from flask import Flask, render_template,  request, jsonify
from langchain.chat_models import ChatAnyscale
from queue import Queue
from threading import Thread
import creds

INPUTMARKER_END = "-- END --"
ANYSCALE_BASE_URL = "https://api.endpoints.anyscale.com/v1"
ANYSCALE_API_KEY = creds.api_key
model_name = "meta-llama/Llama-2-7b-chat-hf"

class StreamingCBH(BaseCallbackHandler):
    def __init__(self, q):
        self.q = q

    def on_llm_new_token(self, token, *, run_id, parent_run_id=None, **kwargs) -> None:
        # When a new token is generated by the language model, put it in the queue.
        self.q.put(token)

    def on_llm_end(self, response, *, run_id, parent_run_id, **kwargs):
        # When the language model finishes generating the response, put the end marker in the queue.
        self.q.put(INPUTMARKER_END)


class LangchainChatAgent():
    def __init__(self, model: str = None):
        # Initialize the message history and the language model with the provided Anyscale API key.
        self.message_history = ChatMessageHistory()
        self.model = model
        self.llm = ChatAnyscale(anyscale_api_base=ANYSCALE_BASE_URL, anyscale_api_key=ANYSCALE_API_KEY, temperature=0,
                                model_name=self.model, streaming=True)

    def process_input(self, user_message: str):
        # Add the user message to the history and prepare a queue for the model's response.
        self.message_history.add_user_message(user_message)
        q = Queue()

        # Start a new thread to predict messages from the language model.
        thread = Thread(target=self.llm.predict_messages, kwargs={
            'messages': self.message_history.messages,
            'callbacks': [StreamingCBH(q)]
        })
        thread.start()
        ai_message = ''
        while True:
            # Collect tokens from the queue until the end marker is received.
            token = q.get()
            if token == INPUTMARKER_END:
                break
            ai_message += token
            yield token

        # Add the complete AI-generated message to the history.
        self.message_history.add_ai_message(ai_message)

def getSubTopics(agent, mainTopic):
    try:
        promptSubTopics = f"The context is preparing a survey to analyze a person's {mainTopic}. Give me 4 focus subtopics for {mainTopic} as an enumerated list. Do not add an explanation. "
        subTopics = "".join(el for el in agent.process_input(promptSubTopics))
        # Split the string by newline characters
        print(subTopics)
        subTopics_lines = subTopics[subTopics.index("1."):].split('\n')
        print(subTopics_lines)
        # Create a dictionary with keys "Q1", "Q2", "Q3", "Q4" and values equal to the string value for each element
        subTopics_dict = {
            'P1': subTopics_lines[0].split('. ', 1)[1].strip(),
            'P2': subTopics_lines[1].split('. ', 1)[1].strip(),
            'P3': subTopics_lines[2].split('. ', 1)[1].strip(),
            'P4': subTopics_lines[3].split('. ', 1)[1].strip()
        }
        return subTopics_dict
    except:
        return
def getQuestions(agent, mainTopic, questionAmount , subTopics):
    try:

        promptQuestion = f"Generate an enumerated list with {questionAmount} generalized questions for a survey on {mainTopic}, slightly touching on {subTopics} . Do not add scale or any other additional information. "
        questions = "".join(el for el in agent.process_input(promptQuestion))
        print(questions)
        questions_lines = questions[questions.index("1."):].split('\n')
        print(questions_lines)
        question_dics = {}
        for i in range(int(questionAmount)):
            question_key = f'Q{i + 1}'
            question_dics[question_key] = questions_lines[i].split('. ', 1)[1].strip()
        return question_dics
    except:
        return


def getAnalysis(agent, mainTopic, questions, answers):
    textQA = ""
    for key in questions:
        textQA += f"{key}: {questions[key]}\n"
        textQA += f"Answer to {key}: {answers[key]}\n"

    promptAnalysis =f"Generalize the person's {mainTopic} (make conclusions beyond the questions and answers).  {textQA}"
    print(promptAnalysis)
    analysis = "".join(el for el in agent.process_input(promptAnalysis))
    return analysis


app = Flask(__name__)
@app.route('/')
def index():
    global llMagent
    llMagent = LangchainChatAgent(model_name)
    return render_template('index.html')

@app.route('/form_topic', methods=['POST'])
def form_topic():
    global configSurvey
    configSurvey= request.json  # Assuming the data is sent as JSON
    subTopics = getSubTopics(llMagent, configSurvey["topic"])
    return jsonify(subTopics)

@app.route('/subprop_form', methods=['POST'])
def subprop_form():
    subProp = request.json  # Assuming the data is sent as JSON
    global questionsGlobal
    questionsGlobal = getQuestions(llMagent, configSurvey["topic"], configSurvey["questionAmount"],subProp)
    return jsonify(questionsGlobal)

@app.route('/answers_form', methods=['POST'])
def answers_form():
    answers = request.json  # Assuming the data is sent as JSON
    return jsonify(getAnalysis(llMagent, configSurvey["topic"], questionsGlobal, answers))


if __name__ == '__main__':
    app.run(debug=True)
